{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ax1r8W0rU8d1"
   },
   "source": [
    "# 线性回归 - Linear Regreesion\n",
    "\n",
    "此Notebook是配合Andrew Ng \"Machine Leanring\"中[线性回归](https://github.com/loveunk/machine-learning-deep-learning-notes/blob/master/machine-learning/linear-regression.md)部分学习使用。\n",
    "\n",
    "测试用python版本为3.6\n",
    "* 机器学习路径：https://github.com/loveunk/machine-learning-deep-learning-notes/\n",
    "* 内容正文综合参考网络资源，使用中如果有疑问请联络：www.kaikai.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgTnKLvOU8d2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\", palette=\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nje4hmN0U8d6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex1data1.txt', names=['population', 'profit']) # 读取数据并赋予列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9p1GK2iU8d8"
   },
   "outputs": [],
   "source": [
    "df.head() # 显示数据前五行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly0Fs3unU8eA"
   },
   "outputs": [],
   "source": [
    "df.info() # 打印df的class信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsQ2uPs6muT7"
   },
   "outputs": [],
   "source": [
    "df.describe() # 打印df的统计信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtj0pJAOU8eE"
   },
   "source": [
    "***\n",
    "# 看下原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ON7EiaK7U8eE"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('population', 'profit', df, size=6, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRWRxgtAU8eH"
   },
   "outputs": [],
   "source": [
    "def get_X(df): # 读取特征\n",
    "#     \"\"\"\n",
    "#     use concat to add intersect feature to avoid side effect\n",
    "#     not efficient for big dataset though\n",
    "#     \"\"\"\n",
    "    ones = pd.DataFrame({'ones': np.ones(len(df))})#ones是m行1列的dataframe\n",
    "    data = pd.concat([ones, df], axis=1)  # 合并数据，根据列合并\n",
    "    return data.iloc[:, :-1].as_matrix()  # 这个操作返回 ndarray,不是矩阵\n",
    "\n",
    "\n",
    "def get_y(df):#读取标签\n",
    "#     '''assume the last column is the target'''\n",
    "    return np.array(df.iloc[:, -1])#df.iloc[:, -1]是指df的最后一列\n",
    "\n",
    "\n",
    "def normalize_feature(df):\n",
    "#     \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\"\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())#特征缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPFqxv_zU8eJ"
   },
   "source": [
    "多变量的假设 h 表示为：${{h}_{\\theta }}\\left( x \\right)={{\\theta }_{0}}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+...+{{\\theta }_{n}}{{x}_{n}}$。\n",
    "\n",
    "这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入${{x}_{0}}=1$，则公式转化为：  ${{h}_{\\theta }}\\left( x \\right)={{\\theta }_{0}x_0}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+...+{{\\theta }_{n}}{{x}_{n}}$。\n",
    "\n",
    "此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是 m*(n+1)。 因此公式可以简化为：${{h}_{\\theta }}\\left( x \\right)={{\\theta }^{T}}X$，其中上标T代表矩阵转置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "on8_khfsU8eQ"
   },
   "source": [
    "# 计算代价函数\n",
    "$$J\\left( \\theta  \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$${{h}_{\\theta }}\\left( x \\right)={{\\theta }^{T}}X={{\\theta }_{0}}{{x}_{0}}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+...+{{\\theta }_{n}}{{x}_{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yomFBPelU8eQ"
   },
   "outputs": [],
   "source": [
    "# 查看数据维度\n",
    "data = df\n",
    "X = get_X(data)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "y = get_y(data)\n",
    "print(y.shape, type(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeTfJ6UyU8eT"
   },
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1]) # X.shape[1]=2, 代表特征数n\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntnGgUoKU8eV"
   },
   "outputs": [],
   "source": [
    "def lr_cost(theta, X, y):\n",
    "    \"\"\" 计算代价函数\n",
    "    X: R(m*n), m 样本数, n 特征数\n",
    "    y: R(m)\n",
    "    theta : R(n), 线性回归的参数\n",
    "    \"\"\"\n",
    "    m = X.shape[0]#m为样本数\n",
    "\n",
    "    inner = X @ theta - y  # R(m*1)，X @ theta等价于X.dot(theta)\n",
    "\n",
    "    # 1*m @ m*1 = 1*1 in matrix multiplication\n",
    "    # but you know numpy didn't do transpose in 1d array, so here is just a\n",
    "    # vector inner product to itselves\n",
    "    square_sum = inner.T @ inner\n",
    "    cost = square_sum / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPFR1bKOU8eY"
   },
   "outputs": [],
   "source": [
    "lr_cost(theta, X, y) # 返回cost的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZ9oOCHrU8eb"
   },
   "source": [
    "# 批量梯度下降 - Batch Gradient Decent\n",
    "$$\\begin{aligned}{{\\theta }_{j}} &:={{\\theta }_{j}}-\\alpha \\frac{\\partial }{\\partial {{\\theta }_{j}}}J\\left( \\theta  \\right) \\\\ &:= {{\\theta }_{j}}-\\alpha \\frac{1}{m} \\sum^{m}_{i=1}\\left( h_\\theta \\left(x^{(i)}\\right) -y^{(i)}  \\right)x^{(i)}_j \\end{aligned}$$\n",
    "注意：对于所有的$j$，需要同时更新$\\theta_j$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_JZqMY3U8ec"
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "  \"\"\"\n",
    "  计算梯度，也就是 J(θ)的偏导数\n",
    "  \"\"\"\n",
    "  m = X.shape[0]\n",
    "\n",
    "  inner = X.T @ (X @ theta - y)  # (m,n).T @ (m, 1) -> (n, 1)，X @ theta等价于X.dot(theta)\n",
    "\n",
    "  return inner / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRexXn6EU8ee"
   },
   "outputs": [],
   "source": [
    "def batch_gradient_decent(theta, X, y, epoch, alpha=0.01):\n",
    "  \"\"\"\n",
    "  批量梯度下降函数。拟合线性回归，返回参数和代价\n",
    "    epoch: 批处理的轮数\n",
    "  \"\"\"\n",
    "  cost_data = [lr_cost(theta, X, y)]\n",
    "  _theta = theta.copy()  # 拷贝一份，不和原来的theta混淆\n",
    "\n",
    "  for _ in range(epoch):\n",
    "    _theta = _theta - alpha * gradient(_theta, X, y)\n",
    "    cost_data.append(lr_cost(_theta, X, y))\n",
    "\n",
    "  return _theta, cost_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lx_dnNrxU8ei"
   },
   "outputs": [],
   "source": [
    "epoch = 500\n",
    "final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCe-db1AU8en"
   },
   "outputs": [],
   "source": [
    "final_theta\n",
    "#最终的theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yigjBbL4U8eq"
   },
   "outputs": [],
   "source": [
    "cost_data\n",
    "# 看下代价数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrMX8THHU8et"
   },
   "outputs": [],
   "source": [
    "# 计算最终的代价\n",
    "lr_cost(final_theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9VOLMPqk2Os"
   },
   "source": [
    "scikit-learn model的预测表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gM2jYk2T0Hv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "x = X[:, 1]\n",
    "f = model.predict(X).flatten()\n",
    "\n",
    "plt.scatter(X[:,1], y, label='Traning Data')\n",
    "plt.plot(x, f, 'r', label='Prediction')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsQw66Y9U8ew"
   },
   "source": [
    "# 代价数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZyEbK4RU8ew"
   },
   "outputs": [],
   "source": [
    "ax = sns.tsplot(cost_data, time=np.arange(epoch+1))\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('cost')\n",
    "plt.show()\n",
    "#可以看到从第二轮代价数据变换很大，接下来平稳了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XhEi6SqU8ez"
   },
   "outputs": [],
   "source": [
    "b = final_theta[0] # intercept，Y轴上的截距\n",
    "m = final_theta[1] # slope，斜率\n",
    "\n",
    "plt.scatter(data.population, data.profit, label=\"Training data\")\n",
    "plt.plot(data.population, data.population*m + b, 'r', label=\"Prediction\")\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67DO0QyBU8e2"
   },
   "source": [
    "# 3- 选修章节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tb-CO9raU8e2"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('ex1data2.txt', names=['square', 'bedrooms', 'price'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "7WBXHMXzU8e6"
   },
   "source": [
    "# 标准化数据\n",
    "最简单的方法是令：\n",
    "\n",
    " \n",
    "\n",
    "其中  是平均值，sn 是标准差。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFLN1gSfU8e7"
   },
   "outputs": [],
   "source": [
    "def normalize_feature(df):\n",
    "#     \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\"\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SL2V8gRmU8e-"
   },
   "outputs": [],
   "source": [
    "data = normalize_feature(raw_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjP2SsV3U8fE"
   },
   "source": [
    "# 2. 多变量批量梯度下降 -  Multi-var batch gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHTA9KcXU8fE"
   },
   "outputs": [],
   "source": [
    "X = get_X(data)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "y = get_y(data)\n",
    "print(y.shape, type(y)) #看下数据的维度和类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUHGSR5jU8fI"
   },
   "outputs": [],
   "source": [
    "alpha = 0.01 #学习率\n",
    "theta = np.zeros(X.shape[1]) #X.shape[1]：特征数n\n",
    "epoch = 500  #迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbC5-6NfU8fL"
   },
   "outputs": [],
   "source": [
    "final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTvLgpImU8fN"
   },
   "outputs": [],
   "source": [
    "sns.tsplot(time=np.arange(len(cost_data)), data = cost_data)\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('cost', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIAeC3aWU8fQ"
   },
   "outputs": [],
   "source": [
    "final_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhv7H1OSTZJs"
   },
   "source": [
    "Scikit-learn 的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dx3gtarBk2Os",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "f = model.predict(X).flatten()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(X[:,1], X[:,2], f, 'r', label='Prediction')\n",
    "ax.scatter(X[:,1], X[:,2], y, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('square')\n",
    "ax.set_ylabel('bedrooms')\n",
    "ax.set_zlabel('price')\n",
    "ax.set_title('square & bedrooms vs. price')\n",
    "#ax.view_init(30, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRrnttEHU8fS"
   },
   "source": [
    "# 3. 学习率 - Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9U-3YXMU8fT"
   },
   "outputs": [],
   "source": [
    "base = np.logspace(-1, -5, num=4)\n",
    "candidate = np.sort(np.concatenate((base, base*3)))\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePELIHY-U8fV"
   },
   "outputs": [],
   "source": [
    "epoch=50\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "for alpha in candidate:\n",
    "    _, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)\n",
    "    ax.plot(np.arange(epoch+1), cost_data, label=alpha)\n",
    "\n",
    "ax.set_xlabel('epoch', fontsize=12)\n",
    "ax.set_ylabel('cost', fontsize=12)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_title('learning rate', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwpPCazZg3Ik"
   },
   "source": [
    "可以看到最合适的learning rate是0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-HlM4onU8fX"
   },
   "source": [
    "# 4. 正规方程 - Normal equation\n",
    "正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac{\\partial }{\\partial {{\\theta }_{j}}}J\\left( {{\\theta }_{j}} \\right)=0$ 。\n",
    " 假设我们的训练集特征矩阵为 X（包含了${{x}_{0}}=1$）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\\theta ={{\\left( {{X}^{T}}X \\right)}^{-1}}{{X}^{T}}y$ 。\n",
    "上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={{X}^{T}}X$，则：${{\\left( {{X}^{T}}X \\right)}^{-1}}={{A}^{-1}}$\n",
    "\n",
    "梯度下降与正规方程的比较：\n",
    "\n",
    "梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型\t\n",
    "\n",
    "正规方程：不需要选择学习率α，一次计算得出，需要计算${{\\left( {{X}^{T}}X \\right)}^{-1}}$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n3)，通常来说当n小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGjQLe7jU8fY"
   },
   "outputs": [],
   "source": [
    "# 正规方程\n",
    "def normalEqn(X, y):\n",
    "    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JB8AH_iU8fa"
   },
   "outputs": [],
   "source": [
    "final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距\n",
    "final_theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfBLlOZnY6Qi"
   },
   "outputs": [],
   "source": [
    "f = final_theta2[0] + final_theta2[1] * X[:,1] + final_theta2[2] * X[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(X[:,1], X[:,2], f, 'r', label='Prediction')\n",
    "ax.scatter(X[:,1], X[:,2], y, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('square')\n",
    "ax.set_ylabel('bedrooms')\n",
    "ax.set_zlabel('price')\n",
    "ax.set_title('square & bedrooms vs. price')\n",
    "#ax.view_init(30, 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.linear_regreesion_v1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (Python 3.6)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
